{
  "total_questions": 14042,
  "correct_answers": 11633,
  "overall_accuracy": 82.84432417034611,
  "subject_stats": {
    "logical_fallacies": {
      "total": 163,
      "correct": 137,
      "accuracy": 84.04907975460122
    },
    "high_school_government_and_politics": {
      "total": 193,
      "correct": 180,
      "accuracy": 93.26424870466322
    },
    "college_mathematics": {
      "total": 100,
      "correct": 98,
      "accuracy": 98.0
    },
    "us_foreign_policy": {
      "total": 100,
      "correct": 90,
      "accuracy": 90.0
    },
    "high_school_microeconomics": {
      "total": 238,
      "correct": 224,
      "accuracy": 94.11764705882352
    },
    "anatomy": {
      "total": 135,
      "correct": 108,
      "accuracy": 80.0
    },
    "public_relations": {
      "total": 110,
      "correct": 85,
      "accuracy": 77.27272727272727
    },
    "high_school_us_history": {
      "total": 204,
      "correct": 179,
      "accuracy": 87.74509803921569
    },
    "management": {
      "total": 103,
      "correct": 90,
      "accuracy": 87.37864077669903
    },
    "business_ethics": {
      "total": 100,
      "correct": 81,
      "accuracy": 81.0
    },
    "machine_learning": {
      "total": 112,
      "correct": 95,
      "accuracy": 84.82142857142857
    },
    "elementary_mathematics": {
      "total": 378,
      "correct": 369,
      "accuracy": 97.61904761904762
    },
    "abstract_algebra": {
      "total": 100,
      "correct": 97,
      "accuracy": 97.0
    },
    "college_physics": {
      "total": 102,
      "correct": 98,
      "accuracy": 96.07843137254902
    },
    "moral_disputes": {
      "total": 346,
      "correct": 263,
      "accuracy": 76.01156069364163
    },
    "virology": {
      "total": 166,
      "correct": 91,
      "accuracy": 54.81927710843374
    },
    "world_religions": {
      "total": 171,
      "correct": 144,
      "accuracy": 84.21052631578947
    },
    "college_computer_science": {
      "total": 100,
      "correct": 89,
      "accuracy": 89.0
    },
    "global_facts": {
      "total": 100,
      "correct": 51,
      "accuracy": 51.0
    },
    "high_school_chemistry": {
      "total": 203,
      "correct": 180,
      "accuracy": 88.66995073891626
    },
    "professional_law": {
      "total": 1534,
      "correct": 868,
      "accuracy": 56.58409387222947
    },
    "high_school_statistics": {
      "total": 216,
      "correct": 193,
      "accuracy": 89.35185185185185
    },
    "professional_medicine": {
      "total": 272,
      "correct": 243,
      "accuracy": 89.33823529411765
    },
    "econometrics": {
      "total": 114,
      "correct": 94,
      "accuracy": 82.45614035087719
    },
    "high_school_biology": {
      "total": 310,
      "correct": 287,
      "accuracy": 92.58064516129032
    },
    "sociology": {
      "total": 201,
      "correct": 169,
      "accuracy": 84.07960199004975
    },
    "high_school_geography": {
      "total": 198,
      "correct": 177,
      "accuracy": 89.39393939393939
    },
    "human_aging": {
      "total": 223,
      "correct": 172,
      "accuracy": 77.13004484304933
    },
    "international_law": {
      "total": 121,
      "correct": 107,
      "accuracy": 88.42975206611571
    },
    "human_sexuality": {
      "total": 131,
      "correct": 112,
      "accuracy": 85.49618320610686
    },
    "computer_security": {
      "total": 100,
      "correct": 87,
      "accuracy": 87.0
    },
    "college_biology": {
      "total": 144,
      "correct": 138,
      "accuracy": 95.83333333333334
    },
    "medical_genetics": {
      "total": 100,
      "correct": 94,
      "accuracy": 94.0
    },
    "clinical_knowledge": {
      "total": 265,
      "correct": 225,
      "accuracy": 84.90566037735849
    },
    "professional_accounting": {
      "total": 282,
      "correct": 213,
      "accuracy": 75.53191489361703
    },
    "high_school_european_history": {
      "total": 165,
      "correct": 145,
      "accuracy": 87.87878787878788
    },
    "high_school_mathematics": {
      "total": 270,
      "correct": 268,
      "accuracy": 99.25925925925925
    },
    "high_school_computer_science": {
      "total": 100,
      "correct": 96,
      "accuracy": 96.0
    },
    "high_school_physics": {
      "total": 151,
      "correct": 134,
      "accuracy": 88.74172185430463
    },
    "philosophy": {
      "total": 311,
      "correct": 237,
      "accuracy": 76.20578778135048
    },
    "jurisprudence": {
      "total": 108,
      "correct": 87,
      "accuracy": 80.55555555555556
    },
    "prehistory": {
      "total": 324,
      "correct": 271,
      "accuracy": 83.64197530864197
    },
    "electrical_engineering": {
      "total": 145,
      "correct": 115,
      "accuracy": 79.3103448275862
    },
    "miscellaneous": {
      "total": 783,
      "correct": 708,
      "accuracy": 90.42145593869732
    },
    "moral_scenarios": {
      "total": 895,
      "correct": 715,
      "accuracy": 79.88826815642457
    },
    "professional_psychology": {
      "total": 612,
      "correct": 493,
      "accuracy": 80.55555555555556
    },
    "college_chemistry": {
      "total": 100,
      "correct": 76,
      "accuracy": 76.0
    },
    "security_studies": {
      "total": 245,
      "correct": 192,
      "accuracy": 78.36734693877551
    },
    "conceptual_physics": {
      "total": 235,
      "correct": 214,
      "accuracy": 91.06382978723404
    },
    "high_school_macroeconomics": {
      "total": 390,
      "correct": 350,
      "accuracy": 89.74358974358975
    },
    "high_school_world_history": {
      "total": 237,
      "correct": 212,
      "accuracy": 89.45147679324894
    },
    "college_medicine": {
      "total": 173,
      "correct": 140,
      "accuracy": 80.92485549132948
    },
    "nutrition": {
      "total": 306,
      "correct": 272,
      "accuracy": 88.88888888888889
    },
    "formal_logic": {
      "total": 126,
      "correct": 115,
      "accuracy": 91.26984126984127
    },
    "astronomy": {
      "total": 152,
      "correct": 140,
      "accuracy": 92.10526315789474
    },
    "marketing": {
      "total": 234,
      "correct": 217,
      "accuracy": 92.73504273504274
    },
    "high_school_psychology": {
      "total": 545,
      "correct": 508,
      "accuracy": 93.21100917431193
    }
  }
}